# ğŸŒ DataLake

Ce projet est une solution complÃ¨te de **DataLake** qui combine :
- Un systÃ¨me de collecte de donnÃ©es via **Kafka** pour l'ingestion de donnÃ©es en temps rÃ©el
- Un stockage persistant sur **MongoDB Atlas**
- Une **API RESTful** pour l'accÃ¨s et la gestion des donnÃ©es

## ğŸ“Š Architecture du DataLake

### Sources de donnÃ©es
- Fichiers CSV/JSON/SQLITE en temps rÃ©el
- Streams de donnÃ©es rÃ©seaux
- DonnÃ©es de transactions
- Posts des rÃ©seaux sociaux

### Pipeline de donnÃ©es
1. **Producers Kafka** : Ingestion des donnÃ©es depuis diffÃ©rentes sources
2. **Topics Kafka** : Organisation des donnÃ©es par type (logs, transactions, posts)
3. **Consumers Kafka** : Traitement et transformation des donnÃ©es
4. **MongoDB Atlas** : Stockage persistant des donnÃ©es structurÃ©es

## ğŸš€ FonctionnalitÃ©s principales

### DataLake
- ğŸ”„ Ingestion en temps rÃ©el via Kafka
- ğŸ“¥ Multiples sources de donnÃ©es supportÃ©es
- ğŸ”„ Transformation des donnÃ©es en vol
- ğŸ’¾ Stockage optimisÃ© sur MongoDB Atlas

### API RESTful
- ğŸ” Authentification par token JWT (`/register`, `/token`)
- ğŸ“‰ Gestion de quota par utilisateur (100 requÃªtes/h)
- ğŸ“ Logs rÃ©seaux (GET / PUT / DELETE)
- ğŸ“¢ RÃ©seaux sociaux (GET / PUT / DELETE)
- ğŸ’³ Transactions clients (GET / PUT / DELETE)
- âœ… AccÃ¨s restreint par rÃ´le (`admin` requis pour certaines routes)

## ğŸ› ï¸ Technologies

### DataLake
- **Apache Kafka** pour l'ingestion de donnÃ©es
- **Kafka Connect** pour la connexion aux sources
- **MongoDB Atlas** pour le stockage
- **Python** pour les consumers/producers

### API
- **FastAPI** + **Pydantic**
- **OAuth2PasswordRequestForm**
- **JWT Token Auth**
- **Quota / Rate limiting** personnalisÃ©

## ğŸ“¦ Installation

```bash
# 1. Cloner le repo
git clone https://github.com/Louube/datalake-api.git
cd datalake-api

# 2. Installer les dÃ©pendances
pip install -r requirements.txt

# 3. Configurer Kafka
# Assurez-vous que Kafka est installÃ© et en cours d'exÃ©cution
# Configurez les variables d'environnement dans .env

# 4. Lancer les consumers Kafka
python kafka/consumers/main.py

# 5. Lancer l'API
uvicorn main:app --reload
```

## ğŸ”§ Configuration

### Variables d'environnement requises
```env
KAFKA_BOOTSTRAP_SERVERS=localhost:9092
MONGO_URI=your_mongodb_uri
JWT_SECRET=your_jwt_secret
```

### Structure des topics Kafka
- `network_logs` : Logs rÃ©seaux
- `social_posts` : Posts des rÃ©seaux sociaux
- `transactions` : Transactions clients
